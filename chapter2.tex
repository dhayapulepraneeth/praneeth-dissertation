\def\baselinestretch{1}

\chapter{Review of Literature}
Cybersecurity is a dynamic field because it responds to the opportunities and threats presented by rapidly developing technologies. Due to the pervasive influence of technology, digital platforms and online services have grown indispensable, leaving users increasingly vulnerable to hacking attempts. Malicious Uniform Resource Locators (URLs), often known as malicious websites, are one of the most popular forms of online assault in the modern day. Individuals, businesses, and even society as a whole are all at danger from these types of attacks, which can vary from phishing and social engineering to virus dissemination.
We explore the vast literature on the important topic of malicious URL detection and categorization in this section. Given these dangers' pervasiveness, it's critical to have reliable and efficient detection systems in place. Cybercriminals are always coming up with new methods to exploit weaknesses, therefore academics and industry professionals must always look for new ways to protect against threats like bad URLs.

\section{Challenges Posed by Malicious URLs:}
While the broad adoption of new technologies and the rise of online services have undoubtedly increased productivity and widened social circles, they have also introduced new and difficult problems, most notably in the field of cybersecurity. In this online environment, malicious Uniform Resource Locators (URLs) have become a serious problem, serving as entry points for cybercrime and other security breaches. These URLs are entry points for hackers to trick unsuspecting consumers and steal their money or personal data. In response to this urgent problem, several security measures have been created to identify and stop the spread of harmful URLs \cite{sahoo2017malicious}.
The usage of blacklists including websites deemed dangerous from the user's perspective is prevalent in conventional methods for identifying bad URLs. These blacklists, or databases of banned websites, are maintained by trusted organisations and devoted individuals. Then, browsers like Chrome, Firefox, and Internet Explorer include these databases for real-time scanning for malicious URLs \cite{tran2014towards}. As a first line of security, this technique alerts users when they attempt to access potentially dangerous websites. However, blacklists' efficacy can be hampered by their inability to encompass the whole, constantly-evolving web; they frequently fail to keep up with the rapid emergence of new dangerous URLs. Despite these challenges, blacklists continue to contribute to users' online safety by offering a mechanism to ward off well-established threats.
The efficacy of blacklists in mitigating malicious URL threats is notably impeded by the dynamic and swiftly evolving nature of web content. The ever-shifting internet landscape has led to an exponential surge in webpage numbers, posing considerable scalability hurdles for traditional blacklist-based approaches. The inherent limitations become even more pronounced due to the inability of web crawlers to penetrate intranet webpages, which necessitate authentication for access. This critical shortcoming further erodes the dependability of blacklists as a comprehensive defence mechanism against the burgeoning array of malicious URLs \cite{alkhudair2020detecting}. Consequently, researchers have turned to novel approaches, particularly machine learning algorithms, to address the limitations of traditional blacklist-based methods.
\cite{kazemian2015comparisons} present an alternative perspective by emphasizing the application of machine learning techniques to tackle the challenge of malicious URL detection. They underscore the shortcomings of blacklists and advocate for leveraging supervised and unsupervised machine learning algorithms to build predictive models for analysing both malicious and safe webpages. Their study introduces three supervised techniques - K-Nearest Neighbor, Support Vector Machine, and Naive Bayes Classifier - and two unsupervised techniques - K-Means and Affinity Propagation. Remarkably, \cite{kazemian2015comparisons} study contributes to the field by introducing K-Means and Affinity Propagation to the realm of malicious webpage detection, highlighting the innovative nature of their approach.
Further support for harnessing machine learning for malicious URL detection is found in the work of \cite{ma2009beyond}. They delve into the essence of automated URL classification as a means to detect malicious websites. By employing statistical methods, Ma and colleagues identify lexical and host-based properties inherent in malicious URLs. Their approach involves extracting and analysing a plethora of features from URLs, leading to the development of highly predictive classifiers. These classifiers exhibit remarkable accuracy, with rates ranging from 95\% to 99\%, thus showcasing the efficacy of machine learning in combating malicious URLs.
The research landscape also extends beyond the traditional realm of websites to encompass other digital platforms, such as social networks like Twitter.  \cite{gangwar2022predictive} shift the focus to Twitter and the challenge of identifying malicious content and user profiles that propagate spam and misinformation. They use machine learning, ensemble, and deep learning approaches to build predictive models using a wide range of characteristics, such as profile-based, content-based, and hybrid features. The outcomes show the efficacy of their models, with accuracy scores above 90\% across a range of performance metrics. This highlights the significance of machine learning for identifying and reducing harmful information across many digital channels.



\section{URL Classification for Security Enhancement:}
While there have been many positive outcomes from the explosion in technical advancement and internet usage, this trend has also accelerated the development of cyber dangers. Among these dangers, malicious Uniform Resource Locators (URLs) have evolved as an effective tool for compromising systems, spreading frauds, and coordinating cyberattacks, all of which can result in significant financial losses and security lapses. These URLs are highly effective in tricking visitors into opening malicious stuff, which might have disastrous results. Researchers have responded by focusing on finding better ways to identify and counteract the dangers posed by malicious URLs.
Blacklists, or databases of websites that have been determined to be hazardous from a user's perspective, have traditionally been used as part of defences against malicious URLs. Blacklists have proved useful in the past, but the dynamic nature of the web and the inaccessibility of intranet pages needing authentication have diminished their use. This means that more complex methods are required, ones that may change with the ever-evolving nature of online dangers. Researchers have begun using machine learning techniques to improve URL categorization and security in response to this need \cite{jeyaraj2020smart}.
In a ground-breaking paper, \cite{afzal2021urldeepdetect}introduce a method they call URLdeepDetect that uses deep learning to instantly analyse and classify URLs. To overcome these restrictions, URLdeepDetect uses semantic vector models and URL encryption in addition to lexical characteristics to analyse URLs. Using Long Short-Term Memory (LSTM) and k-means clustering, the hybrid method combines supervised and unsupervised processes for URL categorization. URLdeepDetect's LSTM and k-means clustering accuracy scores are both extremely high at 99.7 and 98.3 percent, respectively. This innovative approach showcases the potential of deep learning in enhancing URL security through rigorous analysis and classification.
In a similar vein,\cite{le2018urlnet} introduce URLNet, an end-to-end deep learning framework specifically designed for detecting malicious URLs. The researchers highlight the limitations of conventional methods, which struggle to capture semantic meaning and sequential patterns inherent in URL strings. URLNet overcomes these limitations by employing Convolutional Neural Networks (CNNs) on both character and word levels of URL strings, allowing for the comprehensive capturing of semantic information. This innovative approach circumvents the need for substantial manual feature engineering and significantly improves the model's ability to generalize to unseen data.\cite{le2018urlnet}, demonstrate the efficacy of URLNet through extensive experiments on large-scale datasets, showcasing its superiority over existing techniques.
Expanding the scope to social networks,\cite{vo2017revealing} delve into the intricate landscape of malicious retweeter groups. On platforms like Twitter, coordinated retweeting behavior among malicious users has been exploited to distort content visibility for promotional purposes, thereby threatening the authenticity of information dissemination. To address this novel challenge, Vo and colleagues propose the Attractor+ algorithm to extract retweeter groups with similar retweeting behaviors. This approach allows for the identification of coordinated behavior indicative of malicious intent. The proposed method outperforms existing approaches in terms of accurately detecting and revealing malicious retweeter groups, contributing to the burgeoning field of social network security.


\section{Types of Malicious URLs:}
The ever-evolving landscape of cybersecurity has brought to the forefront the significant threat posed by malicious Uniform Resource Locators (URLs). These URLs serve as conduits for a diverse range of cyberattacks, each employing distinct strategies to compromise systems, extract sensitive information, or propagate fraudulent activities. In this section, we delve into the multifaceted nature of malicious URLs, shedding light on the various types of attacks they facilitate, as well as the innovative methodologies employed to detect and mitigate their impact.
 \cite{mamun2016detecting} shed light on the significance of lexical analysis in detecting malicious URLs. One prominent category of malicious URLs is phishing URLs. These URLs mimic legitimate websites, coercing users into divulging sensitive information such as passwords and credit card details. \cite{choi2011detecting} delve deeper into this category, identifying phishing URLs as a subset of malicious URLs that manipulate users' trust to deceive them into sharing confidential data. Another variant of malicious URLs is malware-hosting URLs, which facilitate the distribution of malicious software. Cybercriminals use these URLs to trick users into downloading malware onto their devices. 
The proliferation of approaches based on machine learning has introduced an additional layer of complexity to the environment of harmful URLs. \cite{aljabri2022detecting} bring attention to the variety of assaults and the use of machine learning to categorise and recognise bad URLs. Among the attack types are Drive-By Downloads, where users are infected simply by visiting a compromised webpage, and Cross-Site Scripting (XSS), which exploits vulnerabilities in web applications to inject malicious code. Additionally, SQL Injection attacks manipulate database queries, allowing unauthorized access to sensitive data.
Innovative deep learning approaches have also been leveraged to detect and classify malicious URLs. Classifying harmful URLs is the job of Gated Recurrent Neural Networks (GRNN), as shown by \cite{zhao2019classifying}. Using this strategy, we are able to accurately classify URLs by capturing sequential patterns and contextual information inside them. They released work that refined this approach by using a Convolutional Gated Recurrent Unit Neural Network (CGRU-NN) trained on keywords to detect malicious URLs. Combining convolutional and recurrent neural networks, this model may successfully identify malicious intent by recognising phrases that are indicative of such purpose. Furthermore, harmful URLs amplify their damage by facilitating social engineering attacks, which prey on users' emotional and psychological vulnerabilities. Dishonesty and manipulation are used in these sorts of attacks to convince its targets to act in a certain way. To add insult to injury, despite the fact that URL shorteners are meant to save users time, they may be misused to hide the true nature of a URL, making it harder for consumers to assess whether a link is trustworthy.
\section{Current Approaches and Algorithms:}
With the detection of hazardous URLs being an integral part of contemporary cybersecurity, several methods and algorithms have been developed to mitigate the threats presented by them. In this section, we investigate the algorithms at the heart of the contemporary techniques used to identify and classify harmful URLs. Recently, machine learning has emerged as a strong technique in the area of malicious URL detection, allowing for the development of classifiers that are both accurate and efficient. For this task, \cite{liu2018finding} stress the importance of selecting good classifiers. They look at several classifiers like the Support Vector Machine (SVM), the Random Forest, and the Decision Tree, and they experiment with different parameter settings for each classifier to see which one achieves the best results. This study highlights how important it is to customise classifiers to the specific characteristics of dangerous URLs, recognising the fluid nature of online dangers.
\cite{sahoo2017malicious} provide a thorough review that investigates the field of malicious URL identification using the analytical framework of machine learning. With a meticulous examination of the landscape, the authors elucidate a spectrum of techniques encompassing pivotal aspects like feature extraction, selection, and dimensionality reduction. The study meticulously showcases the gamut of classifiers harnessed in this pursuit, ranging from Support Vector Machines (SVMs) and Naive Bayes to intricate Deep Learning models. This survey distinctly underscores the remarkable versatility of machine learning in tackling the intricate challenge of malicious URL detection. By highlighting the multifaceted nature of the problem and the diversity of methods at play, the authors emphasize the necessity of amalgamating these techniques to construct resilient and precise classifiers that can effectively grapple with the intricacies of this ever-evolving cybersecurity menace.
\cite{darling2015lexical} presents a pioneering lexical approach that harnesses the intrinsic textual attributes of URLs to discern between benign and malicious content. By delving into the fabric of URLs, their method adeptly extracts pertinent features, including domain names and subdomains, which are subsequently harnessed to craft a decision tree classifier. This strategy aptly capitalizes on the linguistic nuances embedded within URLs, providing a judicious avenue for classification that strikes a balance between efficiency and efficacy. The approach holds particular significance in its lightweight nature, offering a pragmatic solution that does not compromise on its ability to discriminate between malicious and non-malicious URLs. \cite{darling2015lexical} innovative approach underscores the potential latent within the textual makeup of URLs, highlighting its value in navigating the complex landscape of URL-based threats.
Because malicious URLs often evolve, adaptive detection methods have become more popular. An adaptive strategy for identifying bogus URLs is provided by \cite{tan2018adaptive} to address the problem of concept drifts, which are characterised by small but significant shifts in the statistical features of data. An ensemble-based architecture is used in their method to combine several detection techniques, with the additional bonus of incremental learning. The strategy remains successful in the face of dynamic threats because it is constantly updated based on new information, making it more resistant to such attacks. This flexibility guarantees the detection system will continue to be robust and accurate despite the ever-changing nature of malicious URLs. \cite{tan2018adaptive} work underscores the critical importance of adaptive strategies in confronting the persistent and mutating nature of cyber threats, offering a promising avenue for bolstering the efficacy of URL detection systems.
A notable endeavor in this direction is the work of\cite{alkhudair2020detecting} and colleagues (2020), who dedicate their efforts to harnessing the latent capabilities residing within URL components like domain names, path segments, and queries, with the aim of bolstering classification accuracy. Their approach transcends conventional boundaries, melding traditional machine learning algorithms with the power of deep learning models. This amalgamation strategically taps into the intricate and discriminative insights intricately woven into the diverse URL components. By embracing this multifaceted approach, \cite{alkhudair2020detecting} strive to unravel the intricate patterns that underlie malicious URLs and benign counterparts, ultimately fortifying the overall efficacy of the classification process. This innovative approach to feature engineering holds the promise of enhancing the precision and adaptability of malicious URL detection mechanisms in the ever-evolving landscape of cyber threats.

\textbf{Malicious URL Detection:}
In today's digital landscape, the proliferation of malicious Uniform Resource Locators (URLs) poses a significant and persistent threat to cybersecurity. Malicious URLs, also known as malicious websites, serve as conduits for a variety of cyberattacks, including phishing, drive-by downloads, and social engineering schemes. These URLs house malicious content designed to trick naive visitors into downloading malware, stealing their personal information, or both. The annual cost of these dangers is in the billions of dollars. Protecting users and infrastructure requires swift action in response to such attacks.
New approaches to finding harmful URLs have been investigated as machine learning has progressed. Using machine learning approaches that take use of URL behaviours and properties,\cite{eshete2011malicious} Xuan, Nguyen, and Tisenko (2020) suggested a detection method. Using big data technologies, they hope to enhance detection skills by pinpointing unusual patterns of activity. Using a machine learning algorithm, big data analytics, and a new collection of URL properties and behaviours, this technique achieves its goals. As shown in experiments, URL properties and behaviour are crucial for improving the capacity to detect rogue URLs.
Traditional machine learning approaches are not the only ones leading to progress in malicious URL identification. When discussing the difficulties of detecting malicious websites,\cite{eshete2011malicious},focused on the issues of efficiency and efficacy.\cite{alkhudair2020detecting} highlighted the significance of detecting malicious URLs in the context of network information security.\cite{kumi2021malicious}explored associative classification as a basis for malicious URL detection. Meanwhile,\cite{abdi2017malicious} harnessed the power of Convolutional Neural Networks (CNN) to detect malicious URLs.
Researchers have embarked on a multifaceted journey to develop effective detection methodologies that mitigate the risks posed by malicious URLs. \cite{sayamber2014malicious}delve into the domain of malicious URL detection and identification, contributing insights to the evolving strategies aimed at safeguarding digital ecosystems. Their work underscores the significance of understanding the structure and components of URLs as a foundational step in the detection process. By dissecting the intricate fabric of URLs, researchers uncover patterns that could unveil the hidden intentions behind seemingly benign links.
\cite{choi2011detecting} take a proactive stance in detecting malicious web links, going beyond the identification of threats to deciphering their attack types. Their study highlights the necessity of not only detecting malicious URLs but also unravelling the diverse tactics employed by cybercriminals. By categorizing attack types, they enhance the granularity of detection mechanisms, enabling a more targeted and informed defence against the ever-evolving threat landscape.
In the domain of malicious URL detection, machine learning methods have attracted significant interest owing to their adaptability and capacity to unveil intricate patterns.\cite{abdi2017malicious} delve into this realm by harnessing the capabilities of convolutional neural networks (CNNs) for detecting malicious URLs. Through the deployment of deep learning techniques, their study serves as a testament to the revolutionary potential of neural networks in unearthing nuanced and intricate attributes within URLs. This achievement is particularly noteworthy as traditional methods often struggle to grapple with the subtleties of such complex patterns. The utilization of CNNs underscores the paradigm shift that deep learning has ushered in, elevating the efficacy of malicious URL detection by enabling the identification of previously elusive features, ultimately enhancing the cyber defense mechanisms against evolving threats.
 \cite{vundavalli2020malicious} contribute significantly to the advancement of malicious URL detection by delving into the domain of supervised machine learning. Their study illuminates the potency of supervised learning techniques as a formidable tool for identifying malicious URLs. By leveraging curated datasets with labeled instances, they tap into the reservoir of prior information, enabling the construction of robust classifiers. Through rigorous training, these models acquire the ability to differentiate between malicious and benign URLs with precision. This approach stands as a testament to the efficacy of harnessing established knowledge to bolster the defense against evolving cyber threats.\cite{vundavalli2020malicious} and colleagues' exploration of supervised machine learning serves as a promising avenue for enhancing the resilience of digital systems against the pervasive menace of malicious URLs.


\section{Related Work:}
\subsection{Natural Language Processing:}
In their study,\cite{Joshi} conducted research on the identification of malicious URLs. They employed a combination of natural language processing (NLP), machine learning techniques, and the FLASK framework. The researchers utilised three distinct methods to encode textual characteristics: count vectorization, TF-IDF, and Hashing vectorization. The researchers conducted approximately 12 experiments to assess the performance of four different machine learning algorithms. After careful evaluation, they discovered that the random forest algorithm achieved the highest accuracy rate of 98.13\%. The best accuracy was achieved when the Hashing vectorization technique was used for random forest. The team created a web application to identify phishing URLs. They used pickle files to save the best model, which was Random Forest with Hashing vectorization, based on the work of \cite{le2018urlnet}.

According to the study conducted by\cite{Seong}, it was found that Intrusion Detection Systems (IDS) used by organisations are generally successful in detecting and preventing known malicious URLs. However, these systems face challenges when it comes to dealing with unknown malicious URLs. In order to tackle this issue, they suggested a system for detecting malicious websites. This system used a lexical approach and included features from Natural Language Processing (NLP). The method they used consisted of representing text and using n-gram models. They were able to achieve a high accuracy rate of 97.1\% by employing Support Vector Machine (SVM). According to \cite{Seong}, the model showed a strong level of robustness. The precision score was 0.97, while the recall score was 0.93.

In their study,\cite{Diri} presented a system for detecting malicious URLs using Natural Language Processing techniques. The Random Forest Algorithm was used in this approach, resulting in an accuracy of 97.2\%  concluded that they used NLP techniques to find domain names in URLs. They experimented with NLP features and Word vectors to show that the NLP-based system was better.

\subsection{Unsupervised Machine Learning:}
In a study conducted by \cite{mahesh2023using}, a survey was carried out to investigate the application of machine learning in identifying malicious URLs. The study pointed out that unsupervised machine learning has limitations when it comes to accurately detecting harmful URLs because there are so many different types of URLs out there. In thier study,\cite{Seong} presented a solution by suggesting a two-phase detection approach which involved the combination of supervised and unsupervised techniques.

In their study,\cite{Mohammed} focused on the detection of phishing URLs. They proposed a ranking-based approach that made use of both lexical and host-based features. The process they introduced involved utilizing K-means clustering to acquire cluster IDs. These cluster IDs were subsequently employed to train a classifier. According to \cite{Mohammed}, the utilization of both unsupervised and supervised techniques resulted in a notable classifier accuracy ranging from 93\% to 98\%.

In their study,\cite{Vanhoenshoven2016DetectingMU} investigated the identification of harmful URLs. They specifically examined unsupervised and supervised learning techniques. The study they conducted included a large dataset with around 6,000,000 URLs. These URLs were a mix of both malicious and non-malicious ones. In their study,\cite{Vanhoenshoven2016DetectingMU}put forward a clustering system based on unsupervised hashing to address this particular challenge.

\section{Summary:}
The chapter focuses on the topic of malicious attacks and the various techniques used to detect and identify malicious URLs. An effective method that was mentioned is called the blacklist approach. In this approach, URLs are compared to a list of predetermined keywords. If any of these keywords are found in the URL, it is blocked. It is crucial to remember that cybercriminals frequently create new malicious URLs in order to avoid detection by blacklisting systems. As a result, there is a potential danger of data breaches occurring. To tackle this issue, it is suggested to utilise machine learning techniques. The approaches mentioned in this context are designed to constantly update themselves by learning from the dataset. They do not depend on predetermined lists of keywords.

